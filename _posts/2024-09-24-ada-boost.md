---
title: "Boosting: AdaBoost"
author: Andrew Benedict
date: 2024-09-24
category: Jekyll
layout: post
---

> **Lưu ý**
>
> Một số thuật ngữ liên quan đến thuật toán trong bài viết này sẽ không được dịch ra tiếng Việt
{:.block-warning}

# 1. Khái niệm về Adaptive Boosting (AdaBoost)
Giả sử bạn đang ôn môn Toán cho kì thi cuối kỳ. Trong lúc ôn thi bạn sẽ giải sai một số bài toán (khó), bạn sẽ làm gì tiếp theo? Tôi tin rằng đa phần sẽ xem lại hoặc làm lại bài toán đó cho tới khi hiểu và giải đúng, đó cũng chính là chiến thuật mà thuật toán Adaptive Boosting sử dụng. "Adaptive" mang nghĩa *thích nghi* hoặc *thích ứng*, ám chỉ base learner của model sẽ dần dần học và hiệu chỉnh theo các sai sót trong quá trình training ở vòng lặp trước đó. Từ từ đã, thuật toán học máy nào mà chẳng hiệu chỉnh để học tốt hơn? Thật ra, với AdaBoost thì từ *adaptive* mang nghĩa đen nhiều hơn khi nó học bám sát theo từng sample mà nó dự đoán sai ở vòng lặp trước đó. Nếu bạn đọc đã xem bài viết trước về Random Forest (RF) thì sẽ biết được có 2 điểm nổi bật của RF. Đó là *huấn luyện song song* và học trên bộ dữ liệu *thu thập ngẫu nhiên có lặp lại (bootstrapping)*. Điều này dẫn đến 2 vấn đề tương ứng mà AdaBoost giải quyết được:
   1. Huấn luyện song song bắt buộc mọi base learner không học được gì lẫn nhau. Lỗi ở learner này có thể gặp lại ở nhiều learners khác.
   2. Bootstrapping không đảm bảo model sẽ giải quyết được những samples mà base learner không học tốt.

AdaBoost chứa các yếu tố có thể giải quyết được 2 vấn đề này cũng như học tốt trên nhiều loại dataset khác nhau.

## 1.1 Kĩ thuật Boosting
Nếu như RF sử dụng kĩ thuật *bagging* làm mấu chốt, thì với AdaBoost ta biết được thêm một nhánh của kĩ thuật *ensemble* đó là *boosting*. Boosting là quá trình ta xây dựng một base learner mới hoặc phát triển thêm base learner đang có dựa trên lỗi hoặc kết quả từ base learner ở vòng lặp trước đó.Nói cách khác, learner thứ $m$ sẽ học dựa trên sai sót của learner thứ $m-1$ hoặc toàn bộ learners trước đó. Các thuật toán dựa trên boosting vì tính chất này mà các learners được xây dựng một cách **tuần tự**. Hệ quả là, trên lý thuyết, các learners được tạo sau sẽ khắc phục được lỗi của learners trước chúng và tựu chung toàn bộ model sẽ đạt kết quả tốt hơn. AdaBoost nổi tiếng với việc vào thời điểm đó là thuật toán có thể dùng một tập hợp các **weak** learners, những learners có kết quả chỉ tốt hơn dự đoán ngẫu nhiên, mà có thể đạt được kết quả của một classifier mạnh.

> **Lưu ý**
>
> Việc sử dụng strong learner làm base learner về mặt lý thuyết vẫn hoạt động. Nhưng model sẽ phải được điều chỉnh khác vì các learners quá mạnh có thể dẫn đến overfitting. Ngoài ra, tại sao phải boost learners mạnh khi nó đã đủ "mạnh" và việc boost weak learners nhanh hơn?
{:.block-warning}


## 1.2. Đánh trọng số mẫu dữ liệu
Với bài Decision Tree trước thì tôi giả định ta không sử dụng *sample weights* (trọng số mẫu dữ liệu) có thể điều chỉnh được bằng biến `sample_weight` trong hàm `fit`. Hay nói cách khác, là ta xem giá trị của mỗi sample là như nhau. Tuy nhiên, sample weights có thể được chỉnh để ảnh hưởng tới giá trị của sample cũng như thay đổi giá trị tách node của cây. Giả sử trong bài toán classification có 2 class $k$ là `0` và `1` với bộ dữ liệu có $N$ điểm, thông thường khi ta tính Gini Impurity cho 1 node $Q$ bất kì và hiện tại đang có $M$ điểm dữ liệu tại node đó thì công thức sẽ là:

$$
    Gini(Q) = 1 - \sum_{k=0}^{1} \sum_{i=1}^{M} \dfrac{1}{M} I(G(x_i)=k)
$$

trong đó $G(x_i)$ trả về class của điểm dữ liệu đó và hàm $I(G(x_i)=k)$ trả về $0$ nếu $x_i$ không thuộc class $k$ và $1$ nếu thuộc class $k$. Như vậy thì công thức thông thường này xem mọi sample có giá trị bằng nhau với việc ta sử dụng $\dfrac{1}{M}$ là giá trị của từng sample. Nếu ta sử dụng sample weights với một array $W$ tương ứng với $w_1$ chứa weight của $x_1$, $w_n$ cho weight của $x_n$ và $\sum_{i=1}^{N}w_i=1$. Như vậy công thức tính Gini mới sẽ là:

$$
    Gini(Q) = 1 - \sum_{k=0}^{1} \sum_{i=1}^{M} w_i I(G(x_i)=k)
$$

Như vậy, khi ta thay đổi sample weight cho một số samples thì cách chọn node của cây cũng bị ảnh hưởng, giả sử ta thay đổi sample weight sao cho class `0` có tổng weight lớn hơn class `1` rất nhiều dù số lượng ít hơn thì Gini cũng sẽ bị thay đổi đáng kể. Nói cách khác, qua sample weight ta có thể làm model chú ý hơn tới những sample "đặc biệt" này. AdaBoost chính xác là dự vào cơ chế này để giúp cây sau hoạt động tốt hơn cây trước. *Thuật toán sẽ giảm weight của các samples "không quan trọng", tức những sample mà learner trước đã đoán đúng, và tăng weight của các samples *quan trọng hơn*, tức những samples mà learner trước đoán sai.* Điều này giống như ví dụ ôn Toán ở trên, bạn có ôn lại (nhiều lần) những bài toán mà mình luôn làm đúng không? Vì chiến thuật đúng hơn vẫn là ôn những bài mà chúng ta làm sai nhiều hơn.

## 1.3. Biểu quyết có trọng số (weighted voting)

Khi ta đã hoàn thành việc training với AdaBoost, thứ ta nhận được không chỉ là một tập hợp các base learners (trong bài này sẽ là các cây quyết định) mà còn một tập hợp các trọng số $\alpha$ tương ứng mỗi cây. AdaBoost đưa ra kết quả cuối vẫn bằng phương pháp biểu quyết (majority voting) như RF, nhưng mỗi quyết định của các cây sẽ bị tác động bởi giá trị $\alpha$ tương ứng (sẽ nói rõ phần sau). Tập hợp giá trị $\alpha$ này có thể hiểu là "sức nặng" (amount of say) mỗi biểu quyết của mỗi cây. Giả sử một trường hợp ngoài đời, bạn cần mua một chiếc ô tô nhưng không biết chọn loại nào nên đi khắp nơi hỏi ý kiến mọi người. Bạn gặp 2 người, một người có 30 năm kinh doanh ô tô và anh ta cho rằng bạn nên mua một chiếc Honda, một người là streamer nổi tiếng, chưa có bằng lái và cho rằng bạn nên mua một chiếc Vinfast. Bạn sẽ tin ai? Rõ ràng người bán xe có lời khuyên có sức nặng lớn hơn, hay trong thuật ngữ AdaBoost là có giá trị $\alpha$ lớn hơn, do kinh nghiệm không thể chối cãi của anh ta. Thuật toán áp dụng nguyên lý y như vậy, base learner có kết quả tốt hơn và mạnh hơn thì đóng góp lớn hơn trong kết quả cuối.


# Tài liệu tham khảo
[Elements of Statistical Learning](https://link.springer.com/book/10.1007/978-0-387-84858-7)
[https://machinelearningtheory.org/docs/Boosting/adaboost/](https://machinelearningtheory.org/docs/Boosting/adaboost/)
